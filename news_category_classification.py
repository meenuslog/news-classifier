# -*- coding: utf-8 -*-
"""Task_2:News_Category_Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cNyWYR48in0WEb_bx5nXpE0maVeUU2Zd

#**News Category Classification**
"""

# Commented out IPython magic to ensure Python compatibility.
import os
import textwrap
from getpass import getpass

# GitHub repo details
username = "meenuslog"
repo_name = "news-classifier"
token = getpass("Enter your GitHub token: ")

# Clone repo
repo_url = f"https://{username}:{token}@github.com/{username}/{repo_name}.git"
!git clone $repo_url

# Path to local repo
repo_path = f"/content/{repo_name}"
os.makedirs(f"{repo_path}/data", exist_ok=True)

# Move dataset into the repo's data directory
!cp "/content/news_category_classification.zip" "{repo_path}/data/news_category_classification.zip"

# Write README.md
readme_content = textwrap.dedent("""
    # AG News Classification

    This project performs news article classification on the News dataset using multiple approaches including classical machine learning (Logistic Regression, Random Forest) and a feedforward neural network built with TensorFlow/Keras.

    ## Dataset

    The News Classification Dataset contains news articles with labels: World, Sports, Business, and Sci/Tech.

    ## Features

    - Data preprocessing with NLTK (stopword removal, lemmatization)
    - Use of pretrained GloVe embeddings for text representation
    - Training and evaluation of Logistic Regression and Random Forest classifiers
    - Building and training a feedforward neural network using TensorFlow/Keras
    - Visualization of top words and word clouds for each category

    ## Requirements

    Install the required libraries using:

    ```bash
    pip install pandas numpy scikit-learn matplotlib seaborn nltk wordcloud tensorflow
    ```
""")

with open(f"{repo_path}/README.md", "w") as f:
    f.write(readme_content)

# Git config and commit
# %cd {repo_path}
!git config --global user.email "emaan.yawer.19@gmail.com"
!git config --global user.name "meenuslog"

!git add .
!git commit -m "Initial commit with README and dataset included"
!git push origin main

"""### Download and unzip dataset, check files"""

!wget https://raw.githubusercontent.com/meenuslog/news-classifier/refs/heads/main/data/news_category_classification.zip -O news.zip

# Unzip the downloaded file
import zipfile

with zipfile.ZipFile("news.zip", "r") as zip_ref:
    zip_ref.extractall("News Dataset")

# Check the contents
import os
print(os.listdir("News Dataset"))

"""###Import Libraries




"""

!pip install wordcloud xgboost lightgbm

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

"""### Load the Data"""

import pandas as pd

train_df = pd.read_csv("/content/news-classifier/News Dataset/train.csv", header=0)
test_df = pd.read_csv("/content/news-classifier/News Dataset/test.csv", header=0)

print("Train shape:", train_df.shape)
print("Test shape:", test_df.shape)

train_df.head()

"""###  Combine Title + Description"""

train_df["Text"] = train_df["Title"].astype(str) + " " + train_df["Description"].astype(str)
test_df["Text"] = test_df["Title"].astype(str) + " " + test_df["Description"].astype(str)

"""###  Text Cleaning Function"""

# Download NLTK resources
nltk.download('stopwords')
nltk.download('wordnet')

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def clean_text(text):
    text = text.lower()
    text = ''.join([c for c in text if c.isalpha() or c.isspace()])  # keep only letters
    tokens = text.split()
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return ' '.join(tokens)

# cleaning
train_df['Clean_Text'] = train_df['Text'].apply(clean_text)
test_df['Clean_Text'] = test_df['Text'].apply(clean_text)

"""### Visualize Most Frequent Words"""

# WordCloud for all training text
all_text = " ".join(train_df['Clean_Text'])
wc = WordCloud(width=800, height=400, background_color='white').generate(all_text)
plt.figure(figsize=(10,5))
plt.imshow(wc, interpolation='bilinear')
plt.axis('off')
plt.title("Word Cloud of All News")
plt.show()

"""### TF-IDF Vectorization"""

vectorizer = TfidfVectorizer(max_features=5000)
X_train = vectorizer.fit_transform(train_df['Clean_Text'])
X_test = vectorizer.transform(test_df['Clean_Text'])

# Target values
y_train = train_df["Class Index"]
y_test = test_df["Class Index"]

"""### Train a Classifier"""

model = LogisticRegression(max_iter=200)
model.fit(X_train, y_train)

"""### Evaluate the Model"""

y_pred = model.predict(X_test)

# Classification report
print(classification_report(y_test, y_pred))

# Confusion matrix
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.show()

"""### Step 9Ô∏è: Map Class Index to Labels"""

# a mapping dictionary
class_mapping = {1: "World", 2: "Sports", 3: "Business", 4: "Sci/Tech"}

# Map for train and test
train_df['Category_Label'] = train_df['Class Index'].map(class_mapping)
test_df['Category_Label'] = test_df['Class Index'].map(class_mapping)

# Quick check
train_df[['Class Index','Category_Label']].head()

"""### Step 10: Per-Category Word Clouds"""

for label in train_df['Category_Label'].unique():
    text_cat = " ".join(train_df[train_df['Category_Label'] == label]['Clean_Text'])
    wc = WordCloud(width=800, height=400, background_color='white').generate(text_cat)
    plt.figure(figsize=(10,5))
    plt.imshow(wc, interpolation='bilinear')
    plt.axis('off')
    plt.title(f"Word Cloud for {label}", fontsize=16)
    plt.show()

"""### Visualize Most Frequent Words Per Category"""

from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns

for label in train_df['Category_Label'].unique():
    # Combine all text for this category
    text_cat = " ".join(train_df[train_df['Category_Label'] == label]['Clean_Text'])

    # Count word frequencies
    words = text_cat.split()
    word_counts = Counter(words)
    top_words = dict(word_counts.most_common(20))

    # Bar plot
    plt.figure(figsize=(10,5))
    sns.barplot(x=list(top_words.values()), y=list(top_words.keys()), palette="viridis")
    plt.title(f"Top 20 Words in {label}", fontsize=16)
    plt.xlabel("Frequency")
    plt.ylabel("Word")
    plt.show()

"""### Train a Simple Feedforward Neural Network (Keras)"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.utils import to_categorical

# One-hot encode labels
y_train_nn = to_categorical(y_train - 1)
y_test_nn = to_categorical(y_test - 1)

# feedforward NN
nn_model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dense(4, activation='softmax')
])

nn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
nn_model.summary()

"""### Evaluate on Test Set"""

nn_model.fit(X_train.toarray(), y_train_nn,
             epochs=5, batch_size=32,
             validation_split=0.2)

loss, accuracy = nn_model.evaluate(X_test.toarray(), y_test_nn)
print(f"Neural Network Test Accuracy: {accuracy:.4f}")

"""### Predictions & Confusion Matrix"""

y_red_nn = nn_model.predict(X_test.toarray())
y_pred_labels = y_pred_nn.argmax(axis=1) + 1

cm_nn = confusion_matrix(y_test, y_pred_labels)
labels = ["World", "Sports", "Business", "Sci/Tech"]

plt.figure(figsize=(8,6))
sns.heatmap(cm_nn, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix (Neural Network)")
plt.show()